{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a752c901",
   "metadata": {},
   "source": [
    "Stemming:\n",
    "Stemming is the process of reducing words to their word stem, base, or root form. \n",
    "It involves removing suffixes or prefixes from words to derive their base form. Stemming algorithms are typically rule-based and can be fast and efficient. However, they may not always produce a valid root word. For example, the word \"running\" \n",
    "might be stemmed to \"run\", but \"flies\" might also be stemmed to \"fli\".\n",
    "\n",
    "Lemmatization:\n",
    "Lemmatization, on the other hand, aims to reduce words to their canonical form or lemma.\n",
    "Unlike stemming, lemmatization considers the context of the word and its part of speech (POS). It involves looking up words in a lexicon (such as WordNet) and applying morphological analysis to determine the lemma. Lemmatization ensures that the resulting word is a valid word in the language.\n",
    "For example, \"ran\" would be lemmatized to \"run\", and \"better\" would be lemmatized to \"good\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4dcd487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-docx in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (1.1.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-docx) (4.9.2)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from python-docx) (4.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install python-docx\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffc4e09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9afc71fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import docx as Document\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aee039fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stemming and lemmatization are different techniques used ran better to reduce words to their root form, but they produce varying results. Lemmatization is better than stemming\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence1 = \"Stemming and lemmatization are different techniques used to reduce words to their root form, but they produce varying results. Lemmatization is better than stemming\"\n",
    "# sentence1 = docx2txt.process('asgn1.docx')\n",
    "\n",
    "sentence1 = open('hello.txt', \"r\").read()\n",
    "sentence2= open('hello2.txt', \"r\").read()\n",
    "sentence1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99535fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stemming', 'and', 'lemmatization', 'are', 'different', 'techniques', 'used', 'ran', 'better', 'to', 'reduce', 'words', 'to', 'their', 'root', 'form', 'but', 'they', 'produce', 'varying', 'results', 'lemmatization', 'is', 'better', 'than', 'stemming']\n",
      "26\n",
      "['helping', 'others', 'is', 'the', 'best', 'thing', 'in', 'the', 'running', 'world', 'keeps', 'you', 'busy', 'and', 'helping', 'always', 'be', 'grateful']\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "def Tokenise(sentence: str):\n",
    "    punctuation=string.punctuation+ '[]{}()<>'\n",
    "    for char in punctuation:\n",
    "        sentence=sentence.replace(char,\" \")\n",
    "#         print(sentence)\n",
    "    sentence= sentence.lower()\n",
    "    tokens=sentence.split()\n",
    "    return tokens\n",
    "\n",
    "tokens=Tokenise(sentence1)\n",
    "print(tokens)\n",
    "print(len(tokens))\n",
    "# token1=word_tokenize(sentence1)\n",
    "token1=Tokenise(sentence1)\n",
    "# token2=word_tokenize(sentence2)\n",
    "token2=Tokenise(sentence2)\n",
    "print(token2)\n",
    "print(len(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03888933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stemming', 'lemmatization', 'different', 'techniques', 'used', 'ran', 'better', 'reduce', 'words', 'root', 'form', 'produce', 'varying', 'results', 'lemmatization', 'better', 'stemming']\n",
      "['helping', 'others', 'best', 'thing', 'running', 'world', 'keeps', 'busy', 'helping', 'always', 'grateful']\n"
     ]
    }
   ],
   "source": [
    "def RemStopWord(token):\n",
    "    stop_word=set(stopwords.words('english'))\n",
    "    filtered=[word for word in token if word not in stop_word]\n",
    "    return filtered\n",
    "\n",
    "token1=RemStopWord(token1)\n",
    "token2=RemStopWord(token2)\n",
    "print(token1)\n",
    "print(token2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02a6af76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('stemming', 'VBG'),\n",
       " ('and', 'CC'),\n",
       " ('lemmatization', 'NN'),\n",
       " ('are', 'VBP'),\n",
       " ('different', 'JJ'),\n",
       " ('techniques', 'NNS'),\n",
       " ('used', 'VBN'),\n",
       " ('ran', 'VBD'),\n",
       " ('better', 'RBR'),\n",
       " ('to', 'TO'),\n",
       " ('reduce', 'VB'),\n",
       " ('words', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('their', 'PRP$'),\n",
       " ('root', 'JJ'),\n",
       " ('form', 'NN'),\n",
       " ('but', 'CC'),\n",
       " ('they', 'PRP'),\n",
       " ('produce', 'VBP'),\n",
       " ('varying', 'VBG'),\n",
       " ('results', 'NNS'),\n",
       " ('lemmatization', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('better', 'JJR'),\n",
       " ('than', 'IN'),\n",
       " ('stemming', 'VBG')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag_list=pos_tag(tokens)\n",
    "pos_tag_list\n",
    "# VBG: Verb, Gerund/Present Participle\n",
    "# NN: Noun, Singular or Mass\n",
    "# JJ: Adjective\n",
    "# NNS: Noun, Plural\n",
    "# VBN: Verb, Past Participle\n",
    "# VB: Verb, Base Form\n",
    "# VBP: Verb, Non-3rd Person Singular Present\n",
    "# RBR: Adverb, Comparative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd464dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemming : stem\n",
      "and : and\n",
      "lemmatization : lemmat\n",
      "are : are\n",
      "different : differ\n",
      "techniques : techniqu\n",
      "used : use\n",
      "ran : ran\n",
      "better : better\n",
      "to : to\n",
      "reduce : reduc\n",
      "words : word\n",
      "to : to\n",
      "their : their\n",
      "root : root\n",
      "form : form\n",
      "but : but\n",
      "they : they\n",
      "produce : produc\n",
      "varying : vari\n",
      "results : result\n",
      "lemmatization : lemmat\n",
      "is : is\n",
      "better : better\n",
      "than : than\n",
      "stemming : stem\n"
     ]
    }
   ],
   "source": [
    "#stemming\n",
    "stemmer=PorterStemmer()\n",
    "stemmed=[]\n",
    "for w in tokens:\n",
    "    print(f\"{w} : {stemmer.stem(w)}\")\n",
    "    stemmed.append(stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0eb66278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemming : stemming\n",
      "and : and\n",
      "lemmatization : lemmatization\n",
      "are : are\n",
      "different : different\n",
      "techniques : technique\n",
      "used : used\n",
      "ran : ran\n",
      "better : better\n",
      "to : to\n",
      "reduce : reduce\n",
      "words : word\n",
      "to : to\n",
      "their : their\n",
      "root : root\n",
      "form : form\n",
      "but : but\n",
      "they : they\n",
      "produce : produce\n",
      "varying : varying\n",
      "results : result\n",
      "lemmatization : lemmatization\n",
      "is : is\n",
      "better : better\n",
      "than : than\n",
      "stemming : stemming\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for w in tokens:\n",
    "    print(f\"{w} : {lemmatizer.lemmatize(w)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ceb62bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalcTF(tokens):\n",
    "    term_freq = {}\n",
    "    for word in tokens:\n",
    "        if word not in term_freq:\n",
    "            term_freq[word]=tokens.count(word)/len(tokens)\n",
    "            \n",
    "    return term_freq\n",
    "\n",
    "CalcTF(tokens)\n",
    "tk1=CalcTF(token1)\n",
    "tk2=CalcTF(token2)\n",
    "# TREM FREq  = occ/total\n",
    "\n",
    "# IDF(t,D)=\n",
    "# log( Number of documents in corpus ∣D∣/Total number of documents containing term t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e67647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def calculate_idf(docList):\n",
    "    all_tokens=[]\n",
    "    for d in docList:\n",
    "        all_tokens+=Tokenise(d)\n",
    "    print(all_tokens)\n",
    "    nodocs=len(docList)\n",
    "    idf=dict()\n",
    "    for t in all_tokens:\n",
    "        f=0\n",
    "        for d in docList:\n",
    "            l=Tokenise(d)\n",
    "            if t in l:\n",
    "                f+=1\n",
    "        idf[t]=math.log(nodocs/f)\n",
    "    return idf,all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "85ad3d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stemming', 'and', 'lemmatization', 'are', 'different', 'techniques', 'used', 'ran', 'better', 'to', 'reduce', 'words', 'to', 'their', 'root', 'form', 'but', 'they', 'produce', 'varying', 'results', 'lemmatization', 'is', 'better', 'than', 'stemming', 'helping', 'others', 'is', 'the', 'best', 'thing', 'in', 'the', 'running', 'world', 'keeps', 'you', 'busy', 'and', 'helping', 'always', 'be', 'grateful']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'stemming': 0.6931471805599453,\n",
       " 'and': 0.0,\n",
       " 'lemmatization': 0.6931471805599453,\n",
       " 'are': 0.6931471805599453,\n",
       " 'different': 0.6931471805599453,\n",
       " 'techniques': 0.6931471805599453,\n",
       " 'used': 0.6931471805599453,\n",
       " 'ran': 0.6931471805599453,\n",
       " 'better': 0.6931471805599453,\n",
       " 'to': 0.6931471805599453,\n",
       " 'reduce': 0.6931471805599453,\n",
       " 'words': 0.6931471805599453,\n",
       " 'their': 0.6931471805599453,\n",
       " 'root': 0.6931471805599453,\n",
       " 'form': 0.6931471805599453,\n",
       " 'but': 0.6931471805599453,\n",
       " 'they': 0.6931471805599453,\n",
       " 'produce': 0.6931471805599453,\n",
       " 'varying': 0.6931471805599453,\n",
       " 'results': 0.6931471805599453,\n",
       " 'is': 0.0,\n",
       " 'than': 0.6931471805599453,\n",
       " 'helping': 0.6931471805599453,\n",
       " 'others': 0.6931471805599453,\n",
       " 'the': 0.6931471805599453,\n",
       " 'best': 0.6931471805599453,\n",
       " 'thing': 0.6931471805599453,\n",
       " 'in': 0.6931471805599453,\n",
       " 'running': 0.6931471805599453,\n",
       " 'world': 0.6931471805599453,\n",
       " 'keeps': 0.6931471805599453,\n",
       " 'you': 0.6931471805599453,\n",
       " 'busy': 0.6931471805599453,\n",
       " 'always': 0.6931471805599453,\n",
       " 'be': 0.6931471805599453,\n",
       " 'grateful': 0.6931471805599453}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf,all_tokens=calculate_idf([sentence1,sentence2])\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9822e7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemming : 0.08154672712469944\n",
      "lemmatization : 0.08154672712469944\n",
      "different : 0.04077336356234972\n",
      "techniques : 0.04077336356234972\n",
      "used : 0.04077336356234972\n",
      "ran : 0.04077336356234972\n",
      "better : 0.08154672712469944\n",
      "reduce : 0.04077336356234972\n",
      "words : 0.04077336356234972\n",
      "root : 0.04077336356234972\n",
      "form : 0.04077336356234972\n",
      "produce : 0.04077336356234972\n",
      "varying : 0.04077336356234972\n",
      "results : 0.04077336356234972\n",
      "helping : 0.12602676010180824\n",
      "others : 0.06301338005090412\n",
      "best : 0.06301338005090412\n",
      "thing : 0.06301338005090412\n",
      "running : 0.06301338005090412\n",
      "world : 0.06301338005090412\n",
      "keeps : 0.06301338005090412\n",
      "busy : 0.06301338005090412\n",
      "always : 0.06301338005090412\n",
      "grateful : 0.06301338005090412\n"
     ]
    }
   ],
   "source": [
    "#tf idf\n",
    "tfidf={}\n",
    "for word in all_tokens:\n",
    "    if word not in tfidf:\n",
    "        if word in tk1:\n",
    "            tfidf[word]=tk1[word]*idf[word]\n",
    "        elif word in tk2:\n",
    "            tfidf[word]=tk2[word]*idf[word]\n",
    "for key,vl in tfidf.items():\n",
    "    print(f\"{key} : {vl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6a57b3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF representation:\n",
      "  (0, 14)\t0.2085144140570748\n",
      "  (0, 21)\t0.2085144140570748\n",
      "  (0, 11)\t0.2085144140570748\n",
      "  (0, 5)\t0.2085144140570748\n",
      "  (0, 15)\t0.2085144140570748\n",
      "  (0, 22)\t0.2085144140570748\n",
      "  (0, 13)\t0.2085144140570748\n",
      "  (0, 2)\t0.4170288281141496\n",
      "  (0, 12)\t0.2085144140570748\n",
      "  (0, 20)\t0.2085144140570748\n",
      "  (0, 18)\t0.2085144140570748\n",
      "  (0, 4)\t0.2085144140570748\n",
      "  (0, 9)\t0.4170288281141496\n",
      "  (0, 17)\t0.4170288281141496\n",
      "  (1, 6)\t0.2773500981126146\n",
      "  (1, 0)\t0.2773500981126146\n",
      "  (1, 3)\t0.2773500981126146\n",
      "  (1, 8)\t0.2773500981126146\n",
      "  (1, 23)\t0.2773500981126146\n",
      "  (1, 16)\t0.2773500981126146\n",
      "  (1, 19)\t0.2773500981126146\n",
      "  (1, 1)\t0.2773500981126146\n",
      "  (1, 10)\t0.2773500981126146\n",
      "  (1, 7)\t0.5547001962252291\n",
      "\n",
      "Feature names:\n",
      "['always' 'best' 'better' 'busy' 'different' 'form' 'grateful' 'helping'\n",
      " 'keeps' 'lemmatization' 'others' 'produce' 'ran' 'reduce' 'results'\n",
      " 'root' 'running' 'stemming' 'techniques' 'thing' 'used' 'varying' 'words'\n",
      " 'world']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Example tokens array (list of strings)\n",
    "tokens_array = token1\n",
    "\n",
    "# Convert the tokens array into a single string (document)\n",
    "document = \" \".join(tokens_array)\n",
    "document2 = \" \".join(token2)\n",
    "\n",
    "# Create a TfidfVectorizer object\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the document and transform the document into TF-IDF representation\n",
    "tfidf_representation = tfidf_vectorizer.fit_transform([document,document2])\n",
    "\n",
    "# Get the feature names (terms) from the vectorizer\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the TF-IDF representation\n",
    "print(\"TF-IDF representation:\")\n",
    "print(tfidf_representation)\n",
    "\n",
    "# Print the feature names (terms)\n",
    "print(\"\\nFeature names:\")\n",
    "print(feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13d5d29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
